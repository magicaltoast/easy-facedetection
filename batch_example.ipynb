{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from insightface.app import FaceAnalysis\n",
    "import cv2\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "model ignore: /home/idk/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "model ignore: /home/idk/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: /home/idk/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "model ignore: /home/idk/.insightface/models/buffalo_l/genderage.onnx genderage\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "model ignore: /home/idk/.insightface/models/buffalo_l/w600k_r50.onnx recognition\n",
      "set det-size: (640, 640)\n"
     ]
    }
   ],
   "source": [
    "app = FaceAnalysis(allowed_modules=[\"detection\"])\n",
    "app.prepare(ctx_id=0, det_size=(640, 640))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "det = app.det_model\n",
    "\n",
    "input_std = det.input_std\n",
    "input_mean = det.input_mean\n",
    "session = det.session\n",
    "output_names = det.output_names\n",
    "input_name = det.input_name\n",
    "fmc = det.fmc\n",
    "feat_stride_fpn = det._feat_stride_fpn\n",
    "use_kps = det.use_kps\n",
    "num_anchors = det._num_anchors\n",
    "threshold = 0.5\n",
    "img = cv2.imread(\"example.jpg\")\n",
    "input_size = 640\n",
    "image = cv2.resize(img, (640, 640))\n",
    "blob = cv2.dnn.blobFromImage(\n",
    "    image,\n",
    "    1.0 / input_std,\n",
    "    (input_size, input_size),\n",
    "    (input_mean, input_mean, input_mean),\n",
    "    swapRB=True,\n",
    ")\n",
    "net_outs = session.run(output_names, {input_name: blob})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance2bbox(points, distance):\n",
    "    x1 = points[:, :, 0] - distance[:, :, 0]\n",
    "    y1 = points[:, :, 1] - distance[:, :, 1]\n",
    "    x2 = points[:, :, 0] + distance[:, :, 2]\n",
    "    y2 = points[:, :, 1] + distance[:, :, 3]\n",
    "    return tf.stack([x1, y1, x2, y2], axis=-1)\n",
    "\n",
    "\n",
    "repeats = 2\n",
    "outputs = list(\n",
    "    map(lambda x: tf.repeat(tf.expand_dims(x, 0), repeats, axis=0), net_outs)\n",
    ")\n",
    "boxes_final = []\n",
    "scores_final = []\n",
    "batch_size = repeats\n",
    "\n",
    "for idx, stride in enumerate(feat_stride_fpn):\n",
    "    scores = tf.squeeze(outputs[idx], -1)\n",
    "\n",
    "    bbox_preds = outputs[idx + fmc]\n",
    "    bbox_preds = bbox_preds * stride\n",
    "    height = input_size // stride\n",
    "    width = input_size // stride\n",
    "    anchor_centers = tf.stack(\n",
    "        tf.meshgrid(tf.range(height), tf.range(width), indexing=\"ij\")[::-1], axis=-1\n",
    "    )\n",
    "    anchor_centers = tf.cast(anchor_centers, tf.float32)\n",
    "    anchor_centers = tf.reshape(anchor_centers * stride, (-1, 2))\n",
    "\n",
    "    if num_anchors > 1:\n",
    "        anchor_centers = tf.reshape(\n",
    "            tf.stack([anchor_centers] * num_anchors, axis=1), (-1, 2)\n",
    "        )\n",
    "\n",
    "    anchor_centers = tf.expand_dims(anchor_centers, 0)\n",
    "    pos_inds = scores >= threshold\n",
    "    bboxes = distance2bbox(anchor_centers, bbox_preds)\n",
    "\n",
    "    pos_scores = tf.ragged.boolean_mask(scores, pos_inds)\n",
    "    pos_bboxes = tf.ragged.boolean_mask(bboxes, pos_inds)\n",
    "\n",
    "    boxes_final.append(pos_bboxes)\n",
    "    scores_final.append(pos_scores)\n",
    "\n",
    "\n",
    "scores_cat = tf.concat(scores_final, 1)\n",
    "bboxes_cat = tf.concat(boxes_final, 1)\n",
    "masks = []\n",
    "\n",
    "scores_good = []\n",
    "bboxes_good = []\n",
    "for i in range(batch_size):\n",
    "    mask = tf.image.non_max_suppression(\n",
    "        bboxes_cat[i], scores_cat[i], max_output_size=100\n",
    "    )\n",
    "    bboxes_good.append(tf.gather(bboxes_cat[i], indices=mask))\n",
    "    scores_good.append(tf.gather(scores_cat[i], indices=mask))\n",
    "\n",
    "\n",
    "res_scores = tf.ragged.stack(scores_good)\n",
    "res_bboxes = tf.ragged.stack(bboxes_good)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=int32, numpy=array([3], dtype=int32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "857db26dd6c269166d7b8aa7786db6678ea642effbb6ba9e7d19a98d9de7d68b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
