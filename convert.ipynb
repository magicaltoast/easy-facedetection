{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from insightface.app import FaceAnalysis\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "import onnx\n",
    "import onnxruntime\n",
    "import sclblonnx as so\n",
    "import numpy as np\n",
    "from dataclasses import dataclass,field\n",
    "from typing import List,Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    example_output: Any\n",
    "    input_std: float\n",
    "    input_mean: float\n",
    "    fmc: int\n",
    "    feat_stride_fpn: List[int]\n",
    "    use_kps: bool\n",
    "    threshold: float\n",
    "    size: int\n",
    "    iou: float\n",
    "    num_anchors:int\n",
    "    max_output_size: int = field(default=100)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_insightface(size, app):\n",
    "        image = np.zeros((1, 3, size, size), dtype=\"float32\")\n",
    "\n",
    "        app.prepare(ctx_id=0, det_size=(size, size))\n",
    "        det = app.det_model\n",
    "        outputs = det.session.run(det.output_names, {det.input_name: image})\n",
    "        print(size)\n",
    "        return Config(\n",
    "            example_output=outputs,\n",
    "            input_std=det.input_std,\n",
    "            input_mean=det.input_mean,\n",
    "            fmc=det.fmc,\n",
    "            feat_stride_fpn=det._feat_stride_fpn,\n",
    "            use_kps=det.use_kps,\n",
    "            threshold=det.det_thresh,\n",
    "            size=size,\n",
    "            num_anchors=det._num_anchors,\n",
    "            iou=det.nms_thresh,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance2bbox(points, distance):\n",
    "    x1 = points[:, 0] - distance[:, 0]\n",
    "    y1 = points[:, 1] - distance[:, 1]\n",
    "    x2 = points[:, 0] + distance[:, 2]\n",
    "    y2 = points[:, 1] + distance[:, 3]\n",
    "    return tf.stack([x1, y1, x2, y2], axis=-1)\n",
    "\n",
    "\n",
    "def distance2kps(points, distance):\n",
    "    c_range = tf.range(0, tf.shape(distance)[1], 2)\n",
    "    px = points[:, 0]\n",
    "    px = tf.reshape(tf.repeat(px, 5), (-1, 5))\n",
    "    px += tf.gather(distance, indices=c_range, axis=-1)\n",
    "    py = points[:, 1]\n",
    "    py = tf.reshape(tf.repeat(py, 5), (-1, 5))\n",
    "    py += tf.gather(distance, indices=c_range + 1, axis=-1)\n",
    "    return tf.stack([px, py], axis=-1)\n",
    "\n",
    "\n",
    "def build_postprocessing(config, size_adjust=True):\n",
    "    inputs = list(\n",
    "        map(\n",
    "            lambda x: Input(x[1].shape, batch_size=1, name=f\"input_{x[0]}\"),\n",
    "            enumerate(config.example_output),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    horizontal_padding_input = Input((1,), batch_size=1, name=\"horizontal_padding\")\n",
    "    vertical_padding_input = Input((1,), batch_size=1, name=\"vertical_padding\")\n",
    "    ratio_input = Input((1,), batch_size=1, name=\"ratio\")\n",
    "\n",
    "    outputs = list(map(lambda x: tf.squeeze(x, 0), inputs))\n",
    "\n",
    "    scores_list = tf.reshape((), (0, 1))\n",
    "    bboxes_list = tf.reshape((), (0, 4))\n",
    "    kps_list = tf.reshape((), (0, 5, 2))\n",
    "\n",
    "    for idx, stride in enumerate(config.feat_stride_fpn):\n",
    "        scores = outputs[idx]\n",
    "        bbox_preds = outputs[idx + config.fmc]\n",
    "        bbox_preds = bbox_preds * stride\n",
    "        height = config.size // stride\n",
    "        width = config.size // stride\n",
    "        anchor_centers = tf.stack(\n",
    "            tf.meshgrid(tf.range(height), tf.range(width), indexing=\"ij\")[::-1], axis=-1\n",
    "        )\n",
    "        anchor_centers = tf.cast(anchor_centers, tf.float32)\n",
    "        anchor_centers = tf.reshape(anchor_centers * stride, (-1, 2))\n",
    "\n",
    "        if config.num_anchors > 1:\n",
    "            anchor_centers = tf.reshape(\n",
    "                tf.stack([anchor_centers] * config.num_anchors, axis=1), (-1, 2)\n",
    "            )\n",
    "\n",
    "        pos_inds = tf.squeeze(scores >= config.threshold)\n",
    "        bboxes = distance2bbox(anchor_centers, bbox_preds)\n",
    "        pos_scores = scores[pos_inds]\n",
    "        pos_bboxes = bboxes[pos_inds]\n",
    "        scores_list = tf.concat([scores_list, pos_scores], 0)\n",
    "        bboxes_list = tf.concat([bboxes_list, pos_bboxes], 0)\n",
    "\n",
    "        if config.use_kps:\n",
    "            kps_preds = outputs[idx + config.fmc * 2] * stride\n",
    "            kpss = distance2kps(anchor_centers, kps_preds)\n",
    "            kpss = kpss[pos_inds]\n",
    "            kps_list = tf.concat([kps_list, kpss], 0)\n",
    "\n",
    "    good = tf.image.non_max_suppression(\n",
    "        bboxes_list,\n",
    "        tf.squeeze(scores_list),\n",
    "        max_output_size=config.max_output_size,\n",
    "        iou_threshold=config.iou,\n",
    "    )\n",
    "\n",
    "    scores = tf.gather(scores_list, indices=good, name=\"scores\")\n",
    "    bboxes = tf.gather(bboxes_list, indices=good, name=\"bboxes\")\n",
    "    key_points = tf.gather(kps_list, indices=good, name=\"keypoints\")\n",
    "    if size_adjust:\n",
    "        vertical_padding = tf.squeeze(vertical_padding_input, axis=0)\n",
    "        horizontal_padding = tf.squeeze(horizontal_padding_input, axis=0)\n",
    "        ratio = tf.squeeze(ratio_input, axis=0)\n",
    "        padding = tf.concat([horizontal_padding, vertical_padding], 0)\n",
    "        key_points = ratio * key_points - padding * ratio\n",
    "        bbox_padding = tf.concat([padding, padding], 0)\n",
    "        bboxes = ratio * bboxes - bbox_padding * ratio\n",
    "\n",
    "        return Model(\n",
    "            [*inputs, ratio_input, horizontal_padding_input, vertical_padding_input],\n",
    "            [scores, bboxes, key_points],\n",
    "        )\n",
    "    else:\n",
    "        return Model(inputs, [scores, bboxes, key_points])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_preprocessing(config, size_adjust=True):\n",
    "    input_img = Input((None, None, 3), batch_size=1, dtype=tf.uint8)\n",
    "    img = tf.squeeze(input_img, name=\"to_remove\")\n",
    "    img = tf.cast(img, tf.float32)\n",
    "\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    img = (img - config.input_mean) * (1 / config.input_std)\n",
    "\n",
    "    if size_adjust:\n",
    "        height = tf.shape(img)[0]\n",
    "        width = tf.shape(img)[1]\n",
    "\n",
    "        vertical_padding = tf.math.maximum(\n",
    "            (config.size - tf.cast((height / width) * config.size, tf.int32)) / 2,\n",
    "            0,\n",
    "        )\n",
    "\n",
    "        vertical_padding = tf.cast(vertical_padding, tf.float32)\n",
    "        vertical_padding = tf.reshape(vertical_padding, (1, 1))\n",
    "\n",
    "        horizontal_padding = tf.math.maximum(\n",
    "            (config.size - tf.cast((width / height) * config.size, tf.int32)) / 2,\n",
    "            0,\n",
    "        )\n",
    "\n",
    "        horizontal_padding = tf.cast(horizontal_padding, tf.float32)\n",
    "        horizontal_padding = tf.reshape(horizontal_padding, (1, 1))\n",
    "\n",
    "        ratio = tf.math.maximum(height, width) / config.size\n",
    "        ratio = tf.cast(ratio, tf.float32, name=\"ratio\")\n",
    "        ratio = tf.reshape(ratio, (1, 1))\n",
    "\n",
    "        img = tf.image.resize_with_pad(img, config.size, config.size)\n",
    "\n",
    "    img = tf.transpose(img, [2, 0, 1])\n",
    "    img = tf.expand_dims(img, 0, name=\"img\")\n",
    "\n",
    "    if size_adjust:\n",
    "        return Model(input_img, [img, ratio, horizontal_padding, vertical_padding])\n",
    "    return Model(input_img, img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf2onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"buffalo_m\"\n",
    "size = 320\n",
    "app = FaceAnalysis(name=name, allowed_modules=[\"detection\"])\n",
    "config = Config.from_insightface(size, app)\n",
    "det = app.det_model\n",
    "\n",
    "size_adjust = False\n",
    "save_intermidiate = True\n",
    "\n",
    "post_processing = build_postprocessing(config, size_adjust=size_adjust)\n",
    "onnx.save(\n",
    "    tf2onnx.convert.from_keras(post_processing, oppset=11)[0], \"post_processing.onnx\"\n",
    ")\n",
    "post_processing = so.graph_from_file(\"post_processing.onnx\")\n",
    "so.rename_output(post_processing, \"tf.compat.v1.gather_6\", \"scores\")\n",
    "\n",
    "if size_adjust:\n",
    "    so.rename_output(post_processing, \"tf.math.subtract_6\", \"key_points\")\n",
    "    so.rename_output(post_processing, \"tf.math.subtract_7\", \"bboxes\")\n",
    "else:\n",
    "    so.rename_output(post_processing, \"tf.compat.v1.gather_7\", \"bboxes\")\n",
    "    so.rename_output(post_processing, \"tf.compat.v1.gather_8\", \"key_points\")\n",
    "if save_intermidiate:\n",
    "    so.graph_to_file(post_processing, \"post_processing.onnx\")\n",
    "\n",
    "pre_processing = build_preprocessing(config, size_adjust=size_adjust)\n",
    "onnx.save(\n",
    "    tf2onnx.convert.from_keras(pre_processing, oppset=11)[0], \"pre_processing.onnx\"\n",
    ")\n",
    "\n",
    "pre_processing = so.graph_from_file(\"pre_processing.onnx\")\n",
    "so.rename_output(pre_processing, \"tf.expand_dims\", \"img\")\n",
    "\n",
    "if size_adjust:\n",
    "    so.rename_output(pre_processing, \"tf.reshape_2\", \"ratio\")\n",
    "    so.rename_output(pre_processing, \"tf.reshape_1\", \"horizontal_padding\")\n",
    "    so.rename_output(pre_processing, \"tf.reshape\", \"vertical_padding\")\n",
    "so.replace_output(pre_processing, \"img\", \"FLOAT\", [1, 3, size, size])\n",
    "\n",
    "if save_intermidiate:\n",
    "    so.graph_to_file(pre_processing, \"pre_processing.onnx\")\n",
    "\n",
    "model = so.graph_from_file(det.model_file)\n",
    "so.replace_input(model, \"input.1\", \"FLOAT\", [1, 3, size, size])\n",
    "\n",
    "for output in det.session.get_outputs():\n",
    "    sq = so.node(\"Unsqueeze\", [output.name], [\"unsquezed\" + output.name], axes=[0])\n",
    "    so.add_node(model, sq)\n",
    "    so.delete_output(model, output.name)\n",
    "    so.add_output(model, \"unsquezed\" + output.name, \"FLOAT\", [1, *output.shape])\n",
    "\n",
    "if save_intermidiate:\n",
    "    so.graph_to_file(model, \"model.onnx\")\n",
    "\n",
    "with_preprocessing = so.merge(\n",
    "    pre_processing, model, io_match=[(\"img\", \"input.1\")], complete=False\n",
    ")\n",
    "\n",
    "if save_intermidiate:\n",
    "    so.graph_to_file(with_preprocessing, \"with_preprocessing.onnx\")\n",
    "m = onnxruntime.InferenceSession(\"with_preprocessing.onnx\")\n",
    "\n",
    "if size_adjust:\n",
    "    match = [\n",
    "        (\"horizontal_padding\", \"horizontal_padding\"),\n",
    "        (\"vertical_padding\", \"vertical_padding\"),\n",
    "        (\"ratio\", \"ratio\"),\n",
    "        *[(x.name, f\"input_{i}\") for i, x in enumerate(m.get_outputs()[3:])],\n",
    "    ]\n",
    "else:\n",
    "    match = [(x.name, f\"input_{i}\") for i, x in enumerate(m.get_outputs())]\n",
    "\n",
    "final = so.merge(with_preprocessing, post_processing, io_match=match, complete=False)\n",
    "if size_adjust:\n",
    "    so.replace_input(final, \"input_1\", \"UINT8\", [\"?\", \"?\", 3])\n",
    "else:\n",
    "    so.replace_input(final, \"input_1\", \"UINT8\", [size, size, 3])\n",
    "so.rename_input(final, \"input_1\", \"image\")\n",
    "\n",
    "for elem in final.node:\n",
    "    if \"to_remove\" in elem.name:\n",
    "        elem.op_type = \"Identity\"\n",
    "        break\n",
    "else:\n",
    "    raise ValueError(\"Failed to remove squeze\")\n",
    "\n",
    "\n",
    "so.graph_to_file(\n",
    "    final,\n",
    "    \"models/\"\n",
    "    + name\n",
    "    + (\"_with_size_adjust_\" if size_adjust else \"_\")\n",
    "    + str(size)\n",
    "    + \".onnx\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "857db26dd6c269166d7b8aa7786db6678ea642effbb6ba9e7d19a98d9de7d68b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
